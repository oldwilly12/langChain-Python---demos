"""
ğŸ“˜ Ejercicio BÃ¡sico 2 - Chatbot con prompt personalizado

ğŸ¯ Objetivo:
Crear un asistente controlado con un prompt estructurado, capaz de responder segÃºn un estilo o rol definido (por ejemplo, como profesor, mÃ©dico o soporte tÃ©cnico).

ğŸ› ï¸ Herramientas utilizadas:
- `PromptTemplate`: Para definir cÃ³mo se estructura la entrada.
- `LLMChain`: Para unir el modelo con el prompt.
- `ChatOpenAI`: Modelo de lenguaje para generar las respuestas.

ğŸ’¡ Ideas de extensiÃ³n:
- Agregar mÃºltiples roles (mÃ©dico, abogado, etc.)
- Cambiar el formato de salida (respuestas en lista, JSON, etc.)
- Incluir instrucciones de comportamiento en el prompt

ğŸ“¦ Requisitos:
- langchain
- openai
- python-dotenv
"""

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.5
)

prompt_template = ChatPromptTemplate([
    ("system", """Eres un asistente experto en explicar conceptos tÃ©cnicos de manera sencilla.
Tu tarea es responder preguntas como si le hablaras a un estudiante de secundaria."""),
    ("user", "{pregunta}")
])

inicio_bot = """
Â¡Hola! Soy tu asistente virtual. Estoy aquÃ­ para ayudarte a entender conceptos tÃ©cnicos de manera sencilla"""
print(inicio_bot)

while True:  
    question = input("Ingresa una pregunta: ")
    if question.lower() in ["exit", "quit", "salir"]:
        print("Saliendo del asistente. Â¡Hasta luego!")
        break
    messages = prompt_template.invoke({"pregunta": question})
    response = llm.invoke(messages)
    print("ğŸ¤– Asistente: ",response.content)
















